# -*- coding: utf-8 -*-
"""NLP INTORDUCTION LEC1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SdXmZZ6c6HxmvEIimgfZu1DMu5w_GFXs
"""

Natural Language Processing, usually shortened as NLP, is a branch of artificial intelligence 
that deals with the interaction between computers and humans using the natural language. 
The ultimate objective of NLP is to read, decipher, understand, 
and make sense of the human languages in a manner that is valuable

Types of NLP
1) NLP
2)DL
#both combine and the common part of NLP/DL is term as DNLP(Deep Natural Language Processing)
#and has a subpart:: seq2sqe(RNN--USE)

Application of NLP
Speech Transcription
Neural Machine Translation
Chatbot
Q&A
Text Summarization
Image Captioning
Vedio Captioning
Optical Character Recognition Converting written or printed text into data.
Speech Recognition Converting spoken words into data.
Machine Translation 
Natural Language Generation
Sentiment Analysis
Semantic Search

Working of NLP
NLP: Two parts
1) NLU: Natural Language Understanding
2) NLG: Natural Language Generation
#working:REF PPT OF NLP

'''
Tokenization breaks the raw text into words, sentences called tokens. 
These tokens help in understanding the context or developing the model for the NLP. 
If the text is split into words using some separation technique it is called word tokenization 
and same separation done for sentences is called sentence tokenization.
'''

#Library Frame work for NLP
1) NLTK:Natural Language ToolKit***
2)TextBlob
3)CoreNLP
4)Gensim
5)SAPCY***
6)plotglot
7)scikit-learn***
8)pattern

#https://spacy.io/
Features
     Non-destructive tokenization
    Named entity recognition
    Support for 59+ languages
    46 statistical models for 16 languages
    Pretrained word vectors
    State-of-the-art speed
    Easy deep learning integration
    Part-of-speech tagging
    Labelled dependency parsing
    Syntax-driven sentence segmentation
    Built in visualizers for syntax and NER
    Convenient string-to-hash mapping
    Export to numpy data arrays
    Efficient binary serialization
    Easy model packaging and deployment
    Robust, rigorously evaluated accuracy

!pip install spacy

import spacy
model=spacy.load('en_core_web_sm')

#lets take some string text
text='"Hello, Mr. George hows you, Hope you are Doing WEll and your pet Max is well?"'
print(text)

doc=model(text)
for token in doc:
  print(token.text, end="|")

text="the Cost of this watch is $15.76"
doc=model(text)
for token in doc:
  print(token)
#here the words are form in token

len(text)

len(doc.vocab)

doc="Hello world and hello ubiverse"
tok=model(doc)
tok[2]
#each word has index

text="Apple to built a Mobile Manufacture company in Hongkong worth $76.8 million"
tok=model(text)
for token in tok:
  print(token.text,end="|")

for i in tok.ents:
  print(i.text+' '+i.label_+' '+str(spacy.explain(i.label_)))
#entity

#lets see chunks
doc="Honda Plan to start a new plant at Khegegaon worth $78.45 billion"
text=model(doc)
for i in text.noun_chunks:
  print(i.text)

from spacy import displacy
text="Apple to built a Mobile Manufacture company in Hongkong worth $76.8 million"
tok=model(text)
displacy.render(tok,style='dep',jupyter=True,options={'distance':110})
#parts of speech and realtion of each word with each other

from spacy import displacy
text="Apple to built a Mobile Manufacture company in Hongkong worth $76.8 million"
tok=model(text)
displacy.render(tok,style='ent',jupyter=True)

# lemmatization
Lemmatization, on the other hand, takes into consideration the morphological analysis of the words. 
To do so, it is necessary to have detailed dictionaries which the algorithm can look through to link 
the form back to its lemma. 
Again, you can see how it works with the same example words.

Mapping from text-word to lemma
help(verb)

text-word       lemma
helping         help(v)
helps           help(v)
helped          help(v)
help            help(v)

https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/
stemming text classification used more
lemm: use in chatbot or bulk data where answer needed is exact

doc=model("hello Ram is helping suresh but suresh didnot helped ram when he needed it")
for i in doc:
   print(i.text,i.lemma_)

#STOP WORD:
In natural language processing, useless words (data), are referred to as stop words. 
Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a 
search engine has been programmed to ignore, both when indexing entries for 
searching and when retrieving them as the result of a search query.

import nltk
import spacy
nltk.download('stopwords')

nlp=spacy.load('en_core_web_sm')
print(nlp.Defaults.stop_words)

len(nlp.Defaults.stop_words)

#how to check weather a word is stop word or not
nlp.vocab['myself'].is_stop

nlp.vocab['abhinash'].is_stop

#suupose you want to add new stop word# word lower case
nlp.Defaults.stop_words.add('abhinash')

#stop_word text on lexeme
nlp.vocab['abhinash'].is_stop=True

len(nlp.Defaults.stop_words)

#check
nlp.vocab['abhinash'].is_stop

#lets remove it
nlp.vocab['abhinash'].is_stop=False
len(nlp.Defaults.stop_words)
#check
nlp.vocab['abhinash'].is_stop

import spacy
import re
import string
import nltk
nltk.download('punkt')
from nltk import word_tokenize,sent_tokenize

#load data
data="The Quick fox jumps over the lazy dog!"

word=word_tokenize(data)
word

#convert each word in lower case
word_low=[]
for i in word:
  word_low.append(i.lower())
print(word_low)

#punctuation 
print(string.punctuation)

#remove the punctuation from text
word_without_pun=[]
pun=[]
for i in word_low :
    if i not in string.punctuation:
        word_without_pun.append(i)
    else:
        pun.append(i)

print(word_without_pun)

#punctaution
print(pun)

#now check on alpbets not any sign
final=[i for i in word_without_pun if i.isalpha()]
final

#lets make a set of stop words from nltk
import nltk
from nltk.corpus import stopwords
sw=set(stopwords.words('english'))

data=[]
for i in final:
   if( i not in sw):
     data.append(i)
print(data)

#check all words and you found that they are not stopwords
#check
for i in data:
  print(nlp.vocab[i].is_stop)